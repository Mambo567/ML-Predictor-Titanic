{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Usuario\\miniconda3\\envs\\prod_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, jsonify, request\n",
    "from sqlalchemy import create_engine, text\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AIzaSyC0lZJh6bVh0MnPPTuCaFKbo-WABKRS-jE'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"GOOGLE_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Muestra: postgresql://<usuario>:<contraseña>@<host>/<base_de_datos>\n",
    "# Protocolo depende de donde estamos trabajando\n",
    "# El usuario depende de como creamnos la estancia\n",
    "# La contraseña también depende de cuando creamos la estancia \n",
    "churro = os.environ[\"churro\"]\n",
    "engine = create_engine(churro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_predictions = pd.DataFrame({\"pclass\": [],\"sex\": [],\"age\": [],\"prediction\": [],\"timestamp\": []})\n",
    "df_predictions.to_sql('predictions', con=engine, if_exists=\"replace\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_predictions = pd.DataFrame({\"pclass\": [1],\"sex\": [1],\"age\": [1],\"prediction\": [1],\"timestamp\": [1]})\n",
    "df_predictions.to_sql('predictions', con=engine, if_exists=\"append\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pclass</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>prediction</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [pclass, sex, age, prediction, timestamp]\n",
       "Index: []"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = '''SELECT * FROM predictions'''\n",
    "test = pd.read_sql(query,con=engine)\n",
    "pd.read_sql(query,con=engine)#.to_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementación de AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Usuario\\miniconda3\\envs\\prod_env\\Lib\\site-packages\\sklearn\\base.py:380: InconsistentVersionWarning: Trying to unpickle estimator LogisticRegression from version 1.5.2 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "with open(\"titanic_model.pkl\", \"rb\") as f:\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Usuario\\miniconda3\\envs\\prod_env\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "pclass = 3\n",
    "sex = 0\n",
    "age = 28\n",
    "input_data = [[pclass, sex, age]]\n",
    "prediction = model.predict(input_data)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No Superviviente'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapita = {0 : \"No Superviviente\", 1 : \"Superviviente\"}\n",
    "prediction = mapita[prediction]\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = [[pclass, sex, age]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(inputs,output):\n",
    "    prompt = f\"\"\"Hola Gemini! Que bueno saludarte!. Te comento: Estoy haciendo una API en la cual se predice la supervivencia de una \n",
    "    persona en el titanic. Mi modelo utiliza los siguientes 3 inputs para hacer la predicción:\n",
    "    pclass: Puede ser 1,2 o 3\n",
    "    sex: es el sexo (0 es hombre, 1 es mujer)\n",
    "    age: Es la edad en años\n",
    "    Por medio de las anteriores variables, mi modelo hace una predicción que da el siguiente output: 'Superviviente' o 'No Superviviente'. Te voy a compartir\n",
    "    los 3 inputs y el output. Quiero que por favor los evalues y cuentes una historia de entre 200 y 300 caracteres donde narres a manera de historia con mucho humor\n",
    "    el porque considerarias que la persona se salvó en caso de que asi haya sido o por que no se salvó en caso de que no se haya salvado, considerando claro, las variables input.\n",
    "    No temas ser creativo y carismatico. Estaria bien que a veces incluyas referencias a la pelicula Titanic con Leonardo Di Caprio, con chistes como \"No cabia en la balsa con Di Caprio\". Te doy ese ejemplo. \n",
    "    IMPORTANTE: El formato de salida de tu respuesta debe ser únicamente y exclusivamente el texto narrado ya que en mi producto daré directamente tu respuesta.\n",
    "    Importante 2: Omite todo tipo de formato enriquecido (markdown, html, etc.) dame solo texto.\n",
    "    Importante 3: Para que el texto no sea muy horizontal y grande, por favor incluye varios saltos de línea para que sea mas visible para los lectores.\n",
    "    Las variables input son las siguientes:\n",
    "    pclass = {input_data[0][0]} sex = {input_data[0][1]} age = {input_data[0][2]}. \n",
    "    La predicción es {output}\n",
    "    \"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hola Gemini! Que bueno saludarte!. Te comento: Estoy haciendo una API en la cual se predice la supervivencia de una \\n    persona en el titanic. Mi modelo utiliza los siguientes 3 inputs para hacer la predicción:\\n    pclass: Puede ser 1,2 o 3\\n    sex: es el sexo (0 es hombre, 1 es mujer)\\n    age: Es la edad en años\\n    Por medio de las anteriores variables, mi modelo hace una predicción que da el siguiente output: \\'Superviviente\\' o \\'No Superviviente\\'. Te voy a compartir\\n    los 3 inputs y el output. Quiero que por favor los evalues y cuentes una historia de entre 200 y 300 caracteres donde narres a manera de historia con mucho humor\\n    el porque considerarias que la persona se salvó en caso de que asi haya sido o por que no se salvó en caso de que no se haya salvado, considerando claro, las variables input.\\n    No temas ser creativo y carismatico. Estaria bien que a veces incluyas referencias a la pelicula Titanic con Leonardo Di Caprio, con chistes como \"No cabia en la balsa con Di Caprio\". Te doy ese ejemplo. \\n    IMPORTANTE: El formato de salida de tu respuesta debe ser únicamente y exclusivamente el texto narrado ya que en mi producto daré directamente tu respuesta.\\n    Importante 2: Omite todo tipo de formato enriquecido (markdown, html, etc.) dame solo texto.\\n    Importante 3: Para que el texto no sea muy horizontal y grande, por favor incluye varios saltos de línea para que sea mas visible para los lectores.\\n    Las variables input son las siguientes:\\n    pclass = 3 sex = 0 age = 28. \\n    La predicción es No Superviviente\\n    '"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = get_prompt(input_data,prediction)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nuestro amigo, de tercera clase, \\ncon 28 años y bigote de galán,\\nintentó hacerse el héroe.\\n\\nPero no, ni por esas. \\nLas balsas estaban llenas y\\nno había cupo para otro Leo\\nde poca monta.\\n\\nEl mar lo reclamó,\\ndejando atrás solo\\nun chaleco mojado.\\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GOOGLE_API_KEY = os.environ[\"GOOGLE_API_KEY\"]\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "model = genai.GenerativeModel('gemini-2.0-flash-exp')\n",
    "def get_text(prompt, model, temperature=0.7, max_output_tokens=1000, top_p=0.95, top_k=40):\n",
    "    \"\"\"Consulta la API de Gemini.\"\"\"\n",
    "    response = model.generate_content(\n",
    "        contents=[{\"parts\": [{\"text\": prompt}]}], # El prompt va dentro de un objeto Content\n",
    "        generation_config=genai.types.GenerationConfig(\n",
    "            temperature=temperature,\n",
    "            max_output_tokens=max_output_tokens,\n",
    "            top_p=top_p,\n",
    "            top_k=top_k\n",
    "        ))\n",
    "    return response.candidates[0].content.parts[0].text\n",
    "get_text(prompt=prompt,model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prod_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
